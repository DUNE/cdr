\chapter{Software and Computing}
\label{ch:detectors-sc}

\section{Computing Infrastructure}
\label{sec:detectors-sc-infrastructure}

\subsection{Data Rates}
\label{sec:detectors-sc-infrastructure-data-rates}
\fixme{Under construction: to be completed before 4/13/2015}

\subsubsection{Assumptions}
According to present baseline design, the Far Detector (Liquid Argon TPC) in DUNE will consist of four identical modules of 10kt each.
For purposes of this document we shall not address possible variations in the design and/or characteristics between
the modules as there is no concrete information developed at this point to support this approach.
\textit{Information presented below corresponds to the ``full detector'', i.e. is effectively normalized to th 40kt mass}.

Where appropriate, we will round off decimal places in certain parameters.

\subsubsection{DUNE Detector Subsystems}
\begin{itemize}
\item Far Detector LAr TPC, Photon Detector(PD)
\item Near Detector (ND) Straw Tracker (STT), Calorimeter(CAL), Muon Detector Resistive Plate Chambers (RPC)
\end{itemize}

In the following, we will itemize estimated data rates for these components and then present the combined estimate.

\subsubsection{Far Detector LAr TPC}
Relevant  LAr TPC parameters:
\begin{itemize}
\item Readout channel count: 1,536,000 (384,000 for each 10kt module)
\item Drift Time: 2ms
\item ADC clock frequency: 2MHz
\item ADC resolution (bits): 12
\end{itemize}
\
Factors affecting data rates:
\begin{itemize}
\item Zero Suppression (ZS)  in the Front-End electronics of the detector
\item Radiolgical and Cosmological Backgrounds as functions of thresholds set for ZS, in different physics domains (cf. beam neutino physics vs Supernova Burst)
\end{itemize}

\subsubsection{Far Detector Photon Detector (PD)}
Relevant  PD parameters:
\begin{itemize}
\item Readout channel count: 
\item ---
\item ---
\item ADC resolution (bits): 12
\end{itemize}
\

\subsubsection{Near Detector Data Rates}
Relevant parameters of the Fine-Grained Tracker (FGT):
\begin{itemize}
\item   Straw Tube Tracker (STT) readout channel count: 215,040
\item STT Drift Time: 120ns
\item ADC clock frequency and resolution (bits): 3ns intervals, 8 bit
\item Calorimeter channel count: 52,224
\item Muon Detector Resistive Plane Chambers (RPC) channel count: 165,888
\end{itemize}
\
Factors affecting data rates:
\begin{itemize}
\item Zero Suppression (ZS)  in the Front-End electronics of the detector
\item Radiolgical and Cosmological Backgrounds as functions of thresholds set for ZS, in different physics domains (cf. beam neutino physics vs Supernova Burst)
\item ADC clock frequency:
\end{itemize}

\subsection{Computing Model}
\label{sec:detectors-sc-infrastructure-computing-model}

\subsubsection{Distributed Computing}

Given the projected volume of  data in DUNE and considerable complexity of the software needed to process it, it may not be optimal to reply on resources located at any single
participating institution or research center. In general, we will take a fully distributed approach to computing, based on experience gained during the operation of the LHC experiments.
This will allow the DUNE Collaboration to better leverage resources and expertise from many of its member institutions and improve the overall long-term scalability of its computing
platform.

DUNE will operate a highly distributed network of federated resources, for both CPU power and storage capability. This will allow for streamlined incorporation of computing facilities
as they become available at member institutions, and thus is particularly amenable to accomodate staged construction and commissioning of the detector subsystems. We will reply
on a modern Workload Management System deployed on top of Grid and Cloud resources to provide computing power to DUNE researchers.

\subsubsection{Raw Data Transmission and Storage Strategy}
FNAL will be the principal data storage center for the experiment. It will serve as a hub where the data from both the Facility (e.g. beam and target)
and the various detector systems (such as the  Far and Near Detectors)  are collected, catalogued and committed to mass storage. This will obviously require transmission of
data over considerable distances (certainly for the Far Detector). In addition, the DAQ systems of the Far Detector are being designed to be located  in the vicinity of
the Far Detector (in the cavern), which results in an additional step of transmitting the data from 4850L to the surface.

Raw data to be collected from the detectors in DUNE are considered ``precious'' due to high cost of operating the both the facility at FNAL
and the detectors that are part of DUNE. This leads to three basic design elements in the data transmission and storage chain:
\begin{itemize}
\item Buffering:
\begin{itemize}
\item Adequate buffers will be provided for the DAQ systems  to mitigate possible downtime of the network connection between 4850L and the surface.
\item Buffers will be provided at the surface facility to mitigate downtime of the network connection between the Far Site and FNAL.
\end{itemize}
\item Robust transmission: data transfer needs to be instrumented with redundant checks (such as checksum calculation), monitoring, error correction and retry logic.
\item Redundant replicas: it is a common industry practice to have a total of three copies of ``precious'' data, which are geographically distributed. This provides protection against catastrophic events (such as natural disasters) at any given data center participating in this scheme, and facilitates rebuilding (``healing'')  lost data should such event does happen.
\end{itemize}



\subsubsection{Data Management}
\label{sec:detectors-sc-infrastructure-computing-model-data-mgt}

Data will be placed into mass storage at FNAL. Along the lines described above, additional copies (replicas) will be distributed to other
computing centers possessing sufficient resources.
A single additional copy does not necessarily need to reside in its entirety on a single data center; the replicas can be ``striped'' across a few data centers if that becomes optimal
at the time of implementation of the Computing Model. We are considering both Brookhaven National Laboratory and NERSC as candidates for the placement of extra replicas.

For data distribution, we will use a combination of managed data movement between sites (such as ``dataset subscription''), primarily for managed production, and a network of XRootD
servers to cache processed data and for analysis.


\section{Physics Software}
\label{sec:detectors-sc-physics-software}

\subsection{Simulation}
\label{sec:detectors-sc-physics-software-simulation}

\subsubsection{Beam Simulation}
\label{sec:detectors-sc-physics-software-simulation-beam}

\subsubsection{Far Detector Simulation}
\label{sec:detectors-sc-physics-software-simulation-fd}


\subsubsection{Near Detector Simulation}
\label{sec:detectors-sc-physics-software-simulation-nd}
 


\subsection{Reconstruction}
\label{sec:detectors-sc-physics-software-reco}


\fixme{What about beam, physics and detector simulations?}

