%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Electronics, Chimneys and DAQ}
\label{sec:detectors-fd-alt-elec}

\subsection{Overview}
\label{sec:fd-alt-elec-ovvw}

The LBNO 20--50-kt detector designs, developed in the LAGUNA-LBNO
design study have channel counts in the range of 500,000 to 1,000,000.
This large number has spawned extensive R\&D over the last few years
into large-scale charge-readout solutions optimized for double-phase detectors.  
The solutions that have been developed provide high integration levels, and significant cost
reduction and performance improvement. They can be fully adopted for
the DUNE far detector double-phase alternate design, which  foresees
153600 channels for each 10-kt far detector module.

The R\&D activities (ongoing since 2006) have focused effort on two
main areas:
\begin {itemize} 
\item{development of cold front-end ASIC electronics, and}
\item{development of low-cost, largely scalable data 
acquisition systems (DAQ) based on modern telecommunications technologies.}
\end{itemize}

One of the goals of the WA105 6$\times$6$\times$6~m$^3$ demonstrator,
a LArTPC with \num{7680} charge-readout channels described in Section~\ref{sec:proto-cern-double}, is to test the
large-scale readout system developed in the LAGUNA-LBNO design
study. \anxdualtdr{}~\cite{WA105_TDR} and~\cite{WA105_SREP} provide detailed descriptions
of the charge-readout electronics, including the cold front-end ASICs
and the DAQ.

The LAGUNA-LBNO design was driven both by cost and by the particular use of the
electronics for the dual-phase readout, which implies larger signals
from the detector relative to single-phase, effectively releasing requirements on noise. 
Section~\ref{sec:detectors-fd-alt-chg-readout} describes
the charge readout system.  Recall that the Large Electron Multipliers (LEMs) 
amplify the ionization charges by at least a factor 20, and that by 
adjusting its voltage, the LEM gain is tunable from there
up to 200. When the charges reach the segmented anode, they are
equally shared among two perpendicular collection views.
The front-end amplifier connected to the anode, given the capacitance
of the anode strips, would have a S/N ratio of 14 at unitary LEM gain. 
Considering a minimal LEM gain of 20, the amplifier provides an
overall S/N ratio of 140. The S/N ratio is boosted by the LEM gain, thus 
implying less stringent requirements on the preamplifier noise.

The electronics consists of front-end amplifiers that are implemented as cryogenic CMOS ASICs
connected to the anode with 50-cm cables; they are completely
accessible from the outside while remaining very low-noise. The
amplifiers must be housed in separate volumes that
are completely distinct from the tank volume
in order to replace them as needed without
contaminating the pure argon inside the cryostat. These volumes are called chimneys.

Flat, 2-m-long cables inside each chimney connect front-end cards
to digitization electronics, housed in microTCA ($\mu$TCA)  crates \cite{mTCA-standard} at the chimney exits, outside the cryostat.
This design provides a way to maintain the front-end at cryogenic temperatures but keep
it accessible, and maintain the digital electronics externally, at room temperature.
 This provides risk mitigation and significant flexibility.

The digitization units in the $\mu$TCA
crates are synchronized with the White Rabbit (WR) time-distribution
standard\cite{WR-standard}, which was originally designed to achieve
sub-ns accuracy.
%
This built-in accuracy, while not a critical aspect of the system
design, is much better than what is needed to align the 400-ns samples
of the charge readout.  WR was adopted for its practical
integration aspects and for cost reduction; its very high timing accuracy is a
bonus.  WR is also used in this design as a dedicated network
network for the trigger distribution. 

The light-readout digitization
electronics (see Section~\ref{sec:detectors-fd-alt-light}) is also
implemented in $\mu$TCA and provides triggers from the
photomultipliers (PMTs) that are distributed to the DAQ via the WR
network. 

Commercial high-bandwidth and high-computing-power back-end
cards are used for event-building and are coupled to a farm for online
processing, which is implemented for event filtering, data
reconstruction, calibrations and data-quality assessment.


\subsection{Front-end Cryogenic Amplifiers and Chimneys}

In the framework of the R\&D related to LAGUNA-LBNO since 2006,
several generations of prototypes of cryogenic ASIC 0.35~microns CMOS
multi-channel preamplifier chips have been developed. 
The capability to operate at cryogenic temperatures means that cables can be shorter 
(in WA105 these cables are just 50~cm long), which reduces the associated 
capacitance for the connection to the detector. It also makes it possible to reach
 an optimal amplifier S/N ratio at a
temperature around 110~K, which can be easily achieved in the GAr at
the top of the cryostat. 


Another significant feature of the design is the ad-hoc designed chimneys, which 
enable the front-end electronics to remain a very short
distance from the detectors in the CRP and accessible for repairs without
opening the cryostat.
Chimneys are
separate, insulated, cylindrical volumes that penetrate the cryostat top, 
their lower half immersed in GAr, their upper half outside the tank at room temperature.
The front-end electronics cards are installed inside the chimneys near the bottom. 
The chimneys are filled with inert gas and have a cooling
system to keep the electronics at the optimal temperature.  A cold feedthrough at
the bottom of each chimney isolates the cards from the inner volume of the
vessel and allows connection from the anode to the electronics;
a warm feedthrough (FT) at the top allows connection to the digitization electronics 
on the outside.

The first ASIC versions were designed principally for the readout of
charge from collection and induction wire planes, and could also handle
bipolar signals. Since 2012 some versions of the dual-phase
have been developed specifically to match the dynamic range of signals
coming from the two collection views of the anode PCB after LEM
amplification. As described in Section~\ref{sec:detectors-fd-alt-chg-readout},  
each collection view is instrumented
with strips of 3.125-mm pitch and 3-m length (150~pF/m capacitance).
For this pitch, simulations of electromagnetic showers predict that a single channel
will collect a maximum 40~MIP. The design of the dual-phase cryogenic ASIC
is based on a LEM minimal gain of 20, which corresponds to
1200~fC for this maximal signal.


Two versions of the dual-phase ASIC chips have been produced
for WA105, both with 16 readout channels. The first version has a
constant gain in the region 0--40~MIP. The second is
characterized by a double-slope gain. This second solution optimizes the
resolution while preserving a large dynamic range. It is characterized
by a high-gain region extending up to signals of 10~MIP, after which the
gain is reduced by a factor of three in order to enable a better overall match for a dynamic
range of 40~MIP.  It provides the best resolution in the MIP
region ($dE/dx$ measurements) without limiting the dynamic range for
showers, which can still reach up to 40~MIP (see
Figure~\ref{fig:FE_ASIC1}). This double-slope regime has been
optimized on the basis of simulations of hadronic and electromagnetic
showers. Both ASIC versions, compatible with the LEM signal dynamics,
are implemented in the CMOS 0.35~$\mu$m technology; they have 16 channels,
18-mW/channel thermal dissipation or less, about 1300 electrons ENC at
250-pF input detector capacitance, and operate with this best S/N ratio at about 110~K.
\begin{cdrfigure}[Dual-phase cryogenic ASIC amplifiers]{FE_ASIC1} 
{Front-end 16 channels cryogenic ASIC amplifier with the double-slope gain implementation}
\includegraphics[width=.3\linewidth]{FE_ASIC2}
\end{cdrfigure}

The implementation of the double-slope gain regime is obtained by
replacing the feedback integration capacitor of the OPAMP with a MOS
capacitance, which changes its value above a certain threshold
voltage. This effect is also present during the discharge phase and it
can be corrected with the inclusion in the feedback loop of an
additional branch with a diode and a resistor designed to keep the RC
value roughly constant during discharge. This branch can be
selected/deselected with an internal switch for all the channels in
the ASIC (see Figure~\ref{fig:FE_doubleslope}).
\begin{cdrfigure}[Double-slope ASIC response]{FE_doubleslope}
{Response of the double-slope ASIC amplifier to progressively larger 
pulses with and without the diode/resistor feedback branch}
\includegraphics[width=\linewidth]{FE_doubleslope}
\end{cdrfigure}

In the design under implementation in WA105 and proposed for DUNE,
there are 640 channels per chimney. The 40 ASIC amplifiers needed for
the readout of each group of 640 channels will be arranged on 10 pairs
of front-end cards plugged into the FT at the bottom of each chimney.
Each front-end card holds two ASIC chips and a few discrete
components. Particular care has been taken in testing several options
(gas discharge tubes, metal oxyde varistors, double diodes) for the
surge-arrestor components, which have to protect the ASICs from
occasional sparks occurring in the CRP.  This study was aimed at
maximizing the protection efficiency, testing the components'
durability for a very high number of sparks and minimizing the input
capacitance seen by the pre-amplifiers. Double-diodes have been
selected as the best solution given their performance and
capacitance. The total dissipation of the front-end electronics will
be about 11.5~W per chimney. This heat source is minor with respect
to the heat conduction from the flat cables going to the digitization
electronics and from the walls of the chimney. The front-end cards are
kept at low temperature by a cooling system installed at the bottom of
chimney 
that compensates for this overall heat flow. The front-end
electronics is coupled to the DAQ system, described in Section~\ref{sec:fd-alt-elec-daq},
that is based on 12-bit ADCs, matching the needed dynamic
range quite well. Figure~\ref{fig:chimneys_scheme} shows the 3D model of the
signal FT chimneys hosting the cryogenic ASIC amplifiers.
\begin{cdrfigure}[3D model of the signal feedthrough chimneys]
{chimneys_scheme}{3D model of the signal feedthrough chimneys}
\includegraphics[width=.5\linewidth]{chimneys_scheme}
\end{cdrfigure}

A signal FT chimney prototype for 320 channels built for the
3$\times$1$\times$1~m$^3$ WA105 prototype is shown in
Figure~\ref{fig:chimneys_proto}.
\begin{cdrfigure}[Prototype of the signal feedthrough chimneys]
{chimneys_proto}{Prototype of the signal feedthrough chimney built 
for the WA015 3$\times$3$\times$1~m$^3$ prototype}
\includegraphics[width=\linewidth]{chimneys_proto}
\end{cdrfigure}

Pairs of cryogenic electronics front-end cards are mounted at the end of
sliding G10 blades which can be extracted from the top of the
chimney. The blades, which also carry the flat cables for the
connections, slide on guides mounted inside the chimney. By
moving the blades, the front-end cards can be plugged/unplugged to/from 
connectors on the top side of the cold FT at
the bottom of the chimney. This FT completely isolates the
chimney from the LAr vessel. Connectors are mounted on the bottom of the FT 
for the 50~cm flat cables coming from the CRPs.

\subsection{Digital Electronics and DAQ Architecture}
\label{sec:fd-alt-elec-daq}


The DAQ system  proposed for the dual-phase DUNE
detector design is based on two industrial standards:
\begin{itemize}
\item MicroTCA ($\mu$TCA) standard for the distributed data network\cite{mTCA-standard}
\item White Rabbit (WR) standard for the distributed clock network\cite{WR-standard}
\end{itemize}

The analog electrical signals from the front-end electronics ASICs transit through the signal chimneys up to the
digitization boards in the $\mu$TCA crates (DAQ L1).
The backplane of each $\mu$TCA crate (a \textit{shelf}) is connected through a 10GbE up-link
to the next level (DAQ L2). The L2 directly connects the $\mu$TCA crates
to FPGA-based high-performance back-end processing boards. 
The design calls for a lossless transmission scheme all the way down to the
back-end processing board, which applies all filtering algorithms and
does the event building. The Huffman lossless algorithm is easy to
implement and typically provides a factor  of 10 compression on LAr events.

Recorded data are sent to a local storage level where Object Storage
Servers (OSS) and MetaData Servers (MDS) are connected with the 
event-building workstations via a 10/40-GbE network (Ethernet or
InfiniBand). In parallel, signals from a high-stability common clock and time
synchronization signals are distributed (using the WR standard) 
to the L1 digitization cards, through a dedicated, deterministic
network. The WR network is also used to transmit the trigger time-stamp signals,
which can be generated either by the PMTs' readout electronics or by
additional sources, e.g., for WA105 operation in the charged particle beam, 
the beam trigger counters. The clock is derived
from a Master Clock generator connected to the WR
Grand-Master switch. WA105 will implement this DAQ scheme, and 
additional details may be found in~\cite{WA105_TDR}.


\subsection{MicroTCA Standard and Applications}

The MicroTCA ($\mu$TCA) standard offers a very compact and easily scalable
architecture to handle a large number of channels at low cost. The
$\mu$TCA or related standards --- such as ATCA or xTCA --- are now
well known in the HEP community and have been integrated into various
designs at CERN (e.g., LHC upgrades), DESY, etc.  $\mu$TCA fulfills
requirements of the telecommunication industry and offers the
ability to interconnect distributed applications while providing a
standard, compact and robust form factor with simplified power supply
management, cooling, and distribution of internal clocks. The backplane of
a $\mu$TCA crate (the $\mu$TCA shelf) accommodates high-speed
serial links; they are arranged in a variety of topologies to support a 
variety of protocols, including Ethernet 1GbE or 10GbE, PCI Express, SRIO,
etc. Use of Ethernet-based solutions is proposed for both data
and clock distribution, through the $\mu$TCA backplanes. This choice
obviously optimizes the connections between the various components of the system.
 Constraints imposed on the data transfer bandwidth point towards use of 
the 10GbE protocol. For the
clock distribution, dedicated lanes on the backplane must be defined by the user. The $\mu$TCA
standard also offers so-called \textit{clock} lanes, which distribute the timing signals to all boards
hosted in the crate and which may be used for any type of signal.

The signal digitization boards plugged into a $\mu$TCA shelf are
called Advanced Mezzanine Cards (AMCs)\cite{picmg-2006}. Each AMC
is connected to one or two $\mu$TCA Carrier Hub (MCH) boards through
the backplane serial links. The MCH provides a central switch function
allowing the AMCs to communicate with each other or with external
systems through an up-link access. The MCH manages both the 10GbE
uplink and the WR bi-directional clock
distribution. Figure~\ref{fig:mTCA-features} provides a sketch of the
backplane layout and its implementation in a particular shelf type selected in the design of the system.

\begin{cdrfigure}[MicroTCA crate organization]{mTCA-features}
{\small Left: global microTCA crate organization. AMCs 
(providing basic ADC functions) are connected to the crate 
controller or MCH which up-links the external systems. A dedicated 
AMC for the clock receives dedicated signals (master clock, trigger 
signals) from the timing distribution system and transcript them onto 
the backplane. Right: backplane layout of the Schroff 11850-015 reference.}
\includegraphics[width=.5\linewidth]{fig-mTCA-sketch.png}\hfill
\includegraphics[width=.4\linewidth]{fig-mTCA-bckpln.png}
\end{cdrfigure}

The production version DAQ designed for the WA105 DAQ is based on the
$\mu$TCA.1 standard, with connections to the user input signals from 
the front side only. These connections are made with VHDCI
cables in order to minimize the number of cables. One $\mu$TCA shelf
is connected to each signal chimney, reading out 640 channels corresponding to 10 AMCs.

%To increase the channels density one may profit of a different $\mu$TCA standard: $\mu$TCA.4. This standard offers the possibility to connect the AMCs hosted in a crate from both the front and rear sides (Figure~\ref{fig:schroff-mTCA-4}). The front card, still called AMC, is connected to the backplane of the shelf, while the read card, so-called $\mu$RTM (Rear Transition Module) is only connected to the AMC. This standard allows to double the number of connections per slot, at the cost of a slightly asymmetric design for the 2 boards. 

Many types of  $\mu$TCA shef are available on the market, e.g.
11850-015 8U from Schroff for $\mu$TCA.1
standard, NATIVE-R9 from NAT for $\mu$TCA.4 standard. The cost of
these items is quickly decreasing due to the fast pace of
developments by the internet providers. These items all have redundant power
supplies, redundant MCHs and offer different segmentations to connect
the AMCs.


%\begin{cdrfigure}[General organization of AMC]{schroff-mTCA-4}{\small Left: general organization of AMC and $\mu$RTM boards in $\mu$TCA.4 standard. %Right: picture comparing $\mu$TCA.4 boards and a VME board.}
%\includegraphics[width=.5\linewidth]{fig-mTCA-4-1.png} \hfill
%\includegraphics[width=.4\linewidth]{fig-mTCA-4-2.png}
%\end{cdrfigure}

The advantage of this architecture is that it limits the DAQ electronics
developments to the AMC only since that is the component that provides
the functionalities for the digitization, data formatting and
compression, event time-stamping and data transfer through the
backplane. For WA105, the AMC is a double-size module (also compatible
with $\mu$TCA.4 standard) with a single input connector and a 10GbE
link to the backplane. The input stage performs the 64-channel
digitization through eight 8-channel, 14-bit ADC chips read out at a
2.5-MHz frequency. The ADC readout sequence is controlled by two FPGAs
that make the data available on a double port memory. Readout of the
data is performed continuously and they are stored in a local
buffer. The recorded samples, each corresponding to a drift window, are
selected in coincidence with the received trigger. When a trigger
occurs, the samples written in the memory can be treated with
compression algorithms (such as Huffman or RLE) or zero-suppression
(if required) and transmitted over the network until the end of the
drift window, which closes the event. These operations are managed by a
third FPGA, which sends the data on the shelf backplane in order that they can be transmitted to L2.  
This readout scheme and hardware implementation have been validated for WA105 on a
Stratix 4 prototype board, shown in Figure~\ref{fig:AMC-bloc-diag}.


\begin{cdrfigure}[Charge readout AMC prototype]{AMC-bloc-diag}
{\small Prototype of AMC, using $\mu$TCA.1 standard, and hosting 
64 ADC channels on a mezzanine board. This prototype is used as a validation 
of the full and final ADC chain in WA105.}
\includegraphics[width=.5\linewidth]{fig-s4am.png}
\end{cdrfigure}

\subsection{Back-end and Event Builder}


A network hierarchical structure is implemented for the back-end and event builder in which all crates are
interconnected to a dedicated back-end FPGA processing board (such as
S5-PCIe-HQ, Figure~\ref{fig:Bittware-board}).

\begin{cdrfigure}[FPGA processing board]{Bittware-board}
{\small FPGA processing board based on Stratix V from Altera. The board 
features a dual QSFP+ cages for 40GigE or 10GigE links, 16 GBytes DDR3 SDRAM, 
72 MBytes QDRII/II+, two SATA connectors and is programmable via OpenCL.}
\includegraphics[width=.4\linewidth]{fig-bittware.jpg}
\end{cdrfigure}

This kind of board is used for massive data processing in many fields
(medical imaging, stock market, etc.) that require parallel
processing with reduced power consumption (it dissipates only 10\% of the power
dissipated by an equivalent CPU for a comparable number of
operations). This particular board has two QSFP+ cages to bring the
data directly to the FPGA for the lowest possible latency. Up to 8$\times$10GbE
links without data loss are available per board.  The board performs
further data processing, filtering and transmission to the highest
level for storage. This type of board is widely used and the present
generation, based on the Altera Stratix V, will evolve to the Aria X
and the Stratix X. Stratix X will be probably available at the time
of construction of the DUNE DAQ system. Programming of the back-end
processing board is achievable through the OpenCL software suite where
a kernel code, on top of a host code, allows programming  the FPGA directly in a
high-level language without a classical VHDL synthesis
chain. OpenCL applications are transparent to the hardware used for
procesing (FPGAs, CPUs, GPUs). This highly flexible feature is fully
adapted to the requirements of large DAQ systems, where conditions
of filtering, event building, etc., may evolve with time.
 
\subsection{Timing Distribution System and White Rabbit (WR) Standard}

The clock distribution will use a parallel, independent
network to distribute the signal from a GPS-disciplined Master Clock down to each $\mu$TCA
shelf, through specific switches. Technically, the WR standard is based
on a combination of Synchronous Ethernet (Sync-E) and Precision Time
Protocol (PTP, IEEE1588), where the Ethernet clock is generated by a
GPS-disciplined clock. At the level of each shelf, this high-accuracy
clock is made available to each AMC through dedicated lines off
the backplane. As discussed before, the $\mu$TCA standard accommodates
special lanes for clock transmission. The trigger signals
(time stamps) are encoded and sent through this dedicated WR network
which has enough bandwidth for this transmission without interfering
with the PTP synchronization signals. The requirements on the
synchronization for the charge readout are quite loose since the
typical readout frequency is of the order of a few MHz. The
requirements for the PMT readout on the contrary are more
stringent. The goal is to provide a nanosecond synchronization at the
level of all L1 elements. This goal is typically achievable with the
WR standard\cite{WR-standard}.


The WR provides an extension to the Ethernet network with Gb/s data
transfer speed and allows for accurate synchronization among the different
network nodes. 
It provides a common clock for the physical layer in the entire
network, allowing sub-nanosecond synchronization accuracy and 20-ps
jitter time. The WR network is designed to host up to thousands of
nodes and to support distance ranges around 10~km using fiber cables. It
ensures that all the Ethernet frames sent are delivered
after no more than a fixed delay  (controlled latency). The order of the frames should be
preserved.  A typical application scheme is displayed
in Figure~\ref{fig:WR_elements}.
\begin{cdrfigure}[White Rabbit network organization]{WR_elements}
{\small Left: general organization of a typical WR network. Right: standalone WR switch.}
\includegraphics[width=.5\linewidth]{fig-WR-network.png}
\includegraphics[width=.3\linewidth]{fig-WRS-rview.png}
\end{cdrfigure}

The WR application in the $\mu$TCA standard is currently engineered 
to easily interconnect $\mu$TCA cards.  The WR  switch
can therefore be connected directly to its different nodes in the same
rack to facilitate 
maintenance and limit the space occupied. 
The implementation scheme is based on the integration of a WR mezzanine board on the MCH
of each shelf. 
In the future  a full integration of the WR within the $\mu$TCA DAQ system would be very powerful. 
The current WR implementation scheme is based on the integration of a WR mezzanine board on the MCH of each shelf. 
The development of this WR MCH is in progress for the WA105
demonstrator (on a MHC produced by the company NAT); the companies
producing the MCH and the WR slave to be integrated show clear interest in this development.