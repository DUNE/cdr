%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Electronics, Chimneys and DAQ}
\label{sec:detectors-fd-alt-elec}

The large number of charge readout channels, needed for the 20-50 kton LAr detectors developed in the LAGUNA-LBNO design study  with channel count in the range of 500'000 to 1'000'000, naturally called during the last years for R\&D efforts in view of the development of large scale charge readout solutions. These are characterized by high-integration levels, significant cost reduction and aims to performance improvement. 

The related R\&D activities, ongoing since 2006, focused on two main axes:
\begin {itemize} 
\item{Development of cold front-end ASIC electronics}
\item{Development of low cost, largely scalable, data acquisition system based on modern telecommunication technologies.}
\end{itemize}

Both efforts aim at improving the effectiveness and the integration level of the complete readout chain and to cost reductions for the large number of channels to be implemented in the detector. One of the goals of the WA105  $6 \times 6 \times  6$ $m^3$ demonstrator including 7680 charge readout channels, is to establish this large scale readout systems developed in the  LAGUNA-LBNO design study. A detailed description of the charge readout electronics including the cold front-end ASICs and the data acquisition system, and the related references, is available in the WA105 Technical Design Report and in the Status report document submitted to the CERN SPSC committee in March 2015 \cite{WA105_TDR}, \cite{WA105_SREP} .

\subsection{Front-end cryogenic amplifiers}

In the framework of the R\&D related to LAGUNA-LBNO, since 2006 several generations of prototypes of ASIC 0.35 microns CMOS multi-channel preamplifier chips operating at cryogenic temperatures, were developed. This implementation offer a double advantage of shortening the length of the cables, and the associated capacitance, for the connection to the detector (in the double-phase design implemented in WA105 these cables are just 50 cm long) and of reaching an optimal S/N ratio at a temperature around 110K which can be easily achieved in the GAr at the top of the cryostat. The other feature of this R\&D program is that this cryogenic front-end electronics, although at very short distance from the detectors in the CRP which in the GAr at the top of the cryostat,  is perfectly accessible at any time without opening the cryostat. This is achieved by implementing the front-end cards at the bottom of some ad-hoc designed chimneys with have a double feed-through (warm and cold). The cold feed-through isolates the cards from the inner volume of the vessel. The chimneys are filled with inert gas and have a colling system to keep the electronics at the optimal temperature.

The first six ASIC versions were generally designed for the readout of charges from collection of induction wire planes, dealing also with bipolar signals. Since 2012 some specific versions for the double phase were produced in order to match the dynamic range of the signals produced by the two collection views of the anode PCB after LEM amplification. In this scheme, each collection  view is instrumented with strips of 3mm pitch and 3 m length (150pF/m capacitance), as foreseen in the WA105 experiment. The design of the double-phase cryogenic ASIC  assumes that the LEM amplification provides a typical gain of 20 and that the maximal signal per channel is equivalent to 40 m.i.p (1200 fC).

There are actually two versions of the double-phase ASIC chips, both with 16 readout channels. A first ASIC version has a constant gain in the region 0-40 m.i.p. The second version is characterized by a double-slope gain. This solution provides an optimization of the resolution over the full dynamic range with a double slope regime. This regime is characterized by a high gain region extending up to 10 m.i.p. signals. After 10 m.i.p. the gain is reduced by a factor 3 in order to overall match a dynamic range of 40 m.i.p. This solution provides the best resolution in the m.i.p. region (dE/dx measurements ) without limiting the dynamic range for showers, which can  still reach up to 40 m.i.p   (see  Figure~\ref{fig:FE_ASIC1} ). This double slope regime has been optimized with simulations of hadronic and electromagnetic showers. Both ASIC versions, compatible with the LEM signals dynamics,  are realized in the CMOS 0.35 microns technology,  have 16 channels, 18mW/channel or less thermal dissipation,  about 1300 electrons ENC at 250pF input detector capacitance and operate with this best S/N ratio figure around  a temperature of 110 K inside ad hoc designed signal chimneys.

\begin{cdrfigure}[Double-phase cryogenic ASIC amplifiers]{FE_ASIC1}{Front-end 16 channels cryogenic ASIC amplifier with the double-slope gain implementation}
\includegraphics[width=.3\linewidth]{FE_ASIC2}
\end{cdrfigure}


The implementation of the double-slope gain regime is obtained by replacing the feedback capacitor of the OPAMP with a MOS capacitance which changes its value above a certain threshold voltage. This effect is also present during the discharge and it can be corrected with the inclusion in the feedback also of branch with a diode and a resistor which keep the RC value constant. This branch  can be selected/deselected with an internal switch for all the channels in the ASIC (see  (Figure~\ref{fig:FE_doubleslope}). ).


\begin{cdrfigure}[Response of double-slope ASIC]{FE_doubleslope}{Response of the double-slope ASIC amplifier to progressively larger pulses with and without the diode/resistor feedback branch}
\includegraphics[width=\linewidth]{FE_doubleslope}
\end{cdrfigure}

In the design under implementation in WA105 and proposed for DUNE, channels are then arranged in groups of 640 per chimney. The 40 ASIC ampliers needed for the readout of each group of 640 channels will be arranged on 10 pairs of FE cards hosted on the feed-through at the bottom of each chimney. 
The front-end cards in the chimney host each one two ASIC chips plus a few discrete components. Particular care has been taken in testing several options (GDT, MOV, Diodes) for the surge arrestor components which has to protect the ASICs from occasional sparks which may be generated in the CRP in order to maximize their protection efficiency, test the components durability for a very high number of sparks and minimize the input capacitance.
The total dissipation of the front-end electronics will be of about 11.5 W per chimney which is compensated, together with conduction heat flow by the cooling system of each chimney.  The front-end electronics is coupled to the DAQ system, described in the following, based on 12 bits ADCs, well matching
the needed dynamic range.

Figure~\ref{fig:chimneys_scheme} shows the 3D model of the signal feedthrough chimneys hosting the cryogenic ASIC amplifiers. A signal FT chimney prototype for 320 channels built for the $3\times1\times 1$ $m^3$ detector is shown in Figure~\ref{fig:chimneys_proto}, the cold electronics FE cards are connected in pairs to sliding G10 blades which can be extracted from the top of the chimney. The FE cards are plugged with the blades on a cold feedthrough at the bottom of the chimney which completely isolates them from the LAr vessel.   

\begin{cdrfigure}[3D model of the signal feedthrough chimneys]{chimneys_scheme}{3D model of the signal feedthrough chimneys}
\includegraphics[width=.5\linewidth]{chimneys_scheme}
\end{cdrfigure}


\begin{cdrfigure}[Prototype of the signal feedthrough chimneys]{chimneys_proto}{Prototype of the signal feedthrough chimneys made for the WA015 $3\times3\times 1$ $m^2$ prototype}
\includegraphics[width=\linewidth]{chimneys_proto}
\end{cdrfigure}

\subsection{Back-end electronics and DAQ global architecture}

The global DAQ system which is proposed for the double-phase DUNE detector design is based on 2 industrial standards~:

\begin{itemize}
\item the MicroTCA ($\mu$TCA) standard for the distributed data network \cite{mTCA-standard}, and
\item the White Rabbit (WR) standard for the distributed clock network \cite{WR-standard}.
\end{itemize}

The global DAQ scheme foresees that raw data transit from the front-end electronics ASICs through the dedicated chimneys and feedthroughs. They are then collected by back-end boards in the $\mu$TCA standard which offers connections through a 10 GbE network (L1). Each $\mu$TCA crate (shelf) is connected through a 10Gbe uplink to the next level (L2). The L2 directly connects the racks to high performance FPGA event builder processing boards. A lossless transmission scheme is therefore foreseen down to the processing board which applies all filtering algorithms and the event building. Recorded data are sent to a local storage level where Object Storage Servers (OSS) and MetaData Servers (MDS) are connected with the event building workstations via a 10/40 GbE network (Ethernet or InfiniBand).\\ 

In parallel, a stable common clock is distributed to L1 and L2, using the White Rabbit standard, through a dedicated, deterministic network, together with the trigger signals (from PMT and beam in the case of the near detector). The clock is derived from a Master Clock generator.\\
This scheme is validated on the WA105 prototype at CERN and many details may be found in \cite{WA105_TDR}.  

\subsection{MicroTCA standard and applications}

The $\mu$TCA standard offers a very compact and easily scalable architecture to manage a large number of channels at low cost. The $\mu$TCA or related standards -- such as ATCA or xTCA -- are now well known in the HEP community and has been integrated in various designs at CERN (LHC upgrades), DESY etc. e.g.  $\mu$TCA fulfills requirements of the telecommunication industry and offers the possibility to interconnect distributed applications while offering a standard, compact and robust form factor with simplified power supply management, cooling and internal clocks distribution.
The backplane if a $\mu$TCA crate -- so-called $\mu$TCA shelf -- is based on high speed serial links arranged in various possible topology to support a large variety of protocols~: Ethernet 1GbE or 10GbE, PCI Express, SRIO etc. The proposal is to use Ethernet-based solutions, both for data and clock distribution, through the $\mu$TCA backplanes. This choice obviously optimizes the connections between the various nodes of the system. Constraints on the data transfer bandwidth imposes to go directly to the 10GbE protocol, which is defined in the $\mu$TCA standard. For the clock distribution, dedicated lanes must be defined by the user. The standard offers so-called clock lanes which distribute all boards within the crate and may be used for any particular signal. 

The boards plugged into a $\mu$TCA shelf are called Advanced Mezzanine Card (AMC)~\cite{picmg-2006}. Each AMC board is connected to one or two MicroTCA Carrier Hub (MCH) through the backplane serial links which provides a central switch function allowing each AMC to communicate with each other or towards external systems through an uplink access. The MCH manages both the 10GbE uplink and the WR bi-directional clock distribution.  
 Figure~\ref{fig:mTCA-features} provides a sketch of the backplane technology and its implementation in one retained shelf reference.

\begin{cdrfigure}[MicroTCA crate organization]{mTCA-features}{\small Left: global microTCA crate organization. AMC cards (providing basic ADC functions) are connected to the crate controller or MCH which uplinks the external systems. A dedicated AMC for the clock receives dedicated signals (masterclock, trigger signals) from the timing distribution system and transcript them onto the backplane. Right: backplane technology of the Schroff 11850-015 reference.}
\includegraphics[width=.5\linewidth]{fig-mTCA-sketch.png}\hfill
\includegraphics[width=.5\linewidth]{fig-mTCA-bckpln.png}
\end{cdrfigure}


The DAQ design and production for WA105 has been using the $\mu$TCA.1 standard with connections to the user input signals from the front side only with VHDCI cables to minimize the number of cables. One $\mu$TCA shelf is connected to each output chimney, reading out 640 channels corresponding to 10 AMC boards. To increase the density one may profit from a quite different $\mu$TCA standard, $\mu$TCA.4, which offers the possibility to enter in a crate both from front and rear sides (Figure~\ref{fig:schroff-mTCA-4}). The front card, still called AMC, is connected to the backplane of the shelf, while the read card, so-called $\mu$RTM (Rear Transition Module) is only connected to the AMC. This standard allows to double the number of connections per slot, at the cost of a slightly asymmetric design for the 2 boards. \\
Many references exist on the market for these various types of shelf~: 11850-015 8U from Schroff for $\mu$TCA.1 standard, NATIVE-R9 from NAT for $\mu$TCA.4 standard. The cost of those items decrease rapidly, profiting from the fast developments of the Internet providers. They have redundant power supplies, redundant MCH and offer different segmentation to connect the AMC boards.  


\begin{cdrfigure}[General organization of AMC]{schroff-mTCA-4}{\small Left: general organization of AMC and $\mu$RTM boards in $\mu$TCA.4 standard. Right: picture comparing $\mu$TCA.4 boards and VME board.}
\includegraphics[width=.5\linewidth]{fig-mTCA-4-1.png} \hfill
\includegraphics[width=.5\linewidth]{fig-mTCA-4-2.png}
\end{cdrfigure}


The great advantage of this design is to reduce the needs of developments to the AMC board only which will basically offer the functions of ADC readout, data formatting and compression, event timestamping and data transfer through the backplane. For WA105, the AMC is a double-size module (also compatible with $\mu$TCA.4 standard) with a single input connector and a 10GbE link to the backplane. The input stage performs the 64 channels digitization through 8 8-channels 14-bits ADC readout at a 2.5MHz frequency. The ADC readout sequence is controlled by 2 FPGA which makes the data available on a double port memory. Readout of the data is performed continuously and the recorded samples, corresponding to a given part of the drift, are selected in coincidence with the received trigger. When a trigger occurs, samples are written in the memory, compression algorithms (such as Huffman or RLE) applied, zero-suppression (if required) performed, until the stop signal occurs to close the event. These operations are managed by a third FPGA, which sends the data on the backplane.  The readout scheme and hardware implementation is validated on a Stratix 4 board as shown in Fig.\ref{fig:AMC-bloc-diag}.

\begin{cdrfigure}[Charge readout AMC board prototype]{AMC-bloc-diag}{\small Prototype of AMC board, using $\mu$TCA.1 standard, and hosting 64 ADC channels on a mezzanine board. This prototype is used as a validation of the full and final ADC chain in WA105.}
\includegraphics[width=.5\linewidth]{fig-s4am.png}
\end{cdrfigure}


\subsection{High-level event builder}


\begin{cdrfigure}[Charge readout AMC board prototype]{Bittware-board}{\small FPGA processing board based on Stratix V from Altera. The board features a dual QSFP+ cages for 40GigE or 10GigE links, 16 GBytes DDR3 SDRAM, 72 MBytes QDRII/II+, two SATA connectors and is programmable via OpenCL.}
\includegraphics[width=.4\linewidth]{fig-bittware.jpg}
\end{cdrfigure}


A network hierarchical structure is implemented where all crates are interconnected to a dedicated FPGA processing board (such as S5-PCIe-HQ, Figure \ref{fig:Bittware-board}). This kind of board is used for massive processing in many fields (medical imaging, stock exchange market etc) which requires parallel processing with reduced power consumption (only 10\% of the power dissipated by a cpu for a comparable number of operations). This particular board has two QSFP+ cages to bring the data direct to the FPGA for lowest possible latency. Up-to 8x10Gbe links w/o data loss are available per board.  The board performs further data processing, filtering and transmission to the highest level for storage. This type of board is widely used and the present generation, based on the Altera Stratix V, will evolve to the Aria X and the Stratix X. This version will be probably available at the time of the construction of the DAQ system.\\
Programming of the processing board is achievable through the OpenCL software suite where a kernel code allows, on top of a host code, to program directly the FPGA without a classical VHDL synthesis chain. This highly flexible feature is fully adapted to the requirements of the large DAQ systems where conditions of filtering, event building etc may evolve with time.\\
 
\subsection{The clock distribution system and the White Rabbit standard}

The idea for the clock distribution is to use a parallel, independent network from a GPS disciplined Master Clock down to each $\mu$TCA shelf, through specific switches. At the level of each shelf, the clock is made available to each AMC board through dedicated lines of the backplane. As was discussed before, the $\mu$TCA standard foresees special lanes for clock transmission. The trigger signals are encoded and sent through this dedicated network.\\
The requirements on the synchronization for the charge readout are quite soft since the typical readout frequency is of the order of a few MHz. The requirements for the PMT readout on the contrary are more stringent. The target is to provide a nanosecond synchronization at the level of all L1 elements. This target is achievable with the White Rabbit (WR) standard \cite{WR-standard}.\\

The WR provides an extension to Ethernet network with Gigabit data transfers and accurate synchronization among its different elements. It provides a common clock for physical layer in the entire network, allowing a nanosecond synchronization accuracy and 20 ps jitter time. The WR network is designed to host up to thousands of nodes and to support distance ranges of 10 km using fiber cables. It ensures that all the Ethernet frames sent are delivered at least after a fixed delay (controlled latency). The order of the frames should be preserved. Technically the WR protocol is built over standard protocols~: Precision Time Protocol (IEEE1588) and Synchronous Ethernet (Sync-E). A typical application scheme is displayed in  (Figure~\ref{fig:WR_elements}).\\

\begin{cdrfigure}[General organization of White Rabbit network]{WR_elements}{\small Left: general organization of a typical WR network. Right: standalone WR switch.}
\includegraphics[width=.5\linewidth]{fig-WR-network.png}
\includegraphics[width=.5\linewidth]{fig-WRS-rview.png}
\end{cdrfigure}

The WR applications in $\mu$TCA standard is targeted for the moment to easily interconnect different $\mu$TCA cards between them. The switch can therefore be connected directly to its different nodes in the same RACK to improve the maintenance and the space. This future development may be very powerful to a full integration within the $\mu$TCA DAQ system. For the moment the scheme requires the development of a mezzanine board to be plugged on the MCH of each shelf. This development is under progress for the WA105 prototype (NAT-MCH) with a clear interest of commercial companies.   
 